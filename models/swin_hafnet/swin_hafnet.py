import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision.ops.deform_conv import deform_conv2d
from timm.models.swin_transformer import SwinTransformerBlock
from typing import Tuple, Optional
import math # Import math for trunc_normal_
import scipy.stats as stats
from timm.layers import DropPath
from einops import rearrange
import warnings
from constants import *

def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
  # tensor: A torch.Tensor, typically weights of layer
    # From https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/layers/weight_init.py
    # def norm_cdf(x): # Cummulative normal distribution
    #     return (1. + math.erf((x - mu) / (std * math.sqrt(2.)))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] bounds",
                      stacklevel=2)
    with torch.no_grad():
        # Values are generated by truncated normal distribution
        l = stats.norm.cdf(a, loc=mean, scale=std)
        u = stats.norm.cdf(b, loc=mean, scale=std)
        tensor.uniform_(2 * l - 1, 2 * u - 1)
        tensor.erfinv_()
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)
        tensor.clamp_(min=a, max=b)
        return tensor

# Helper class for Depthwise Convolution (from the reference, assuming it exists or is needed)
class DWConv(nn.Module):
    def __init__(self, dim=768):
        super(DWConv, self).__init__()
        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)

    def forward(self, x, H, W):
        B, N, C = x.shape
        x = x.transpose(1, 2).view(B, C, H, W)
        x = self.dwconv(x)
        x = x.flatten(2).transpose(1, 2)
        return x

#-------------------Encoder Blocks-------------------#
class PatchPartition(nn.Module):
    """
    Input shape: (B, C, H, W)
    Output shape: (B, C*patch_size*patch_size, H/patch_size, W/patch_size)
    E.g. 4x downsampling if patch_size=4
    """
    def __init__(self, patch_size=4):
        super(PatchPartition, self).__init__()
        self.patch_size = patch_size

    def forward(self, x):
        """
        x: (B, C, H, W)
        """
        B, C, H, W = x.shape
        assert H % self.patch_size == 0 and W % self.patch_size == 0, "H and W must be divisible by patch_size."

        x = x.view(B, C, H // self.patch_size, self.patch_size, W // self.patch_size, self.patch_size)
        x = x.permute(0, 2, 4, 1, 3, 5).contiguous()  # (B, H/ps, W/ps, C, ps, ps)
        x = x.view(B, H // self.patch_size, W // self.patch_size, -1)  # (B, H/ps, W/ps, C*ps*ps)
        return x

class LinearEmbed(nn.Module):
    """
    Linear Embedding layer that flattens the patch partition to a linear embedding.
    Input shape: (B, H/patch_size, W/patch_size, C * patch_size * patch_size)
    Output shape: (B, embed_dim, H/patch_size, W/patch_size)
    """
    def __init__(self, in_channels=CHANNELS * PATCH_SIZE * PATCH_SIZE, embed_dim=EMBEDDING_DIM, norm_layer=nn.LayerNorm):
        super().__init__()
        self.proj = nn.Linear(in_channels, embed_dim)
        self.norm = norm_layer(embed_dim) if norm_layer is not None else nn.Identity()

    def forward(self, x):
        """
        Args:
            x: Input tensor of shape (B, H, W, C * patch_size * patch_size)
        Returns:
            x: Embedded tensor of shape (B, embed_dim, H, W)
        """
        x = self.proj(x)  # (B, H, W, embed_dim)
        x = self.norm(x)  # (B, H, W, embed_dim)
        x = x.permute(0, 3, 1, 2).contiguous()  # (B, embed_dim, H, W)
        return x

# Swin Transformer Block is imported from timm, so no need to redefine it here.

class PatchMerging(nn.Module):
    """
    Patch Merging Layer that downsamples the input feature map.
    Input shape: (B, C, H, W)
    Output shape: (B, 2*C, H/2, W/2)
    """
    def __init__(self, dim):
        super(PatchMerging, self).__init__()
        self.dim = dim
        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)
        self.norm = nn.LayerNorm(4 * dim)
    def forward(self, x):
        """
        x: (B, C, H, W)
        """
        B, C, H, W = x.shape
        assert H % 2 == 0 and W % 2 == 0, "H and W must be even."

        # Convert to (B, H, W, C) for patch merging operations
        x = x.permute(0, 2, 3, 1).contiguous()  # (B, H, W, C)

        x0 = x[:, 0::2, 0::2, :]  # (B, H/2, W/2, C)
        x1 = x[:, 1::2, 0::2, :]  # (B, H/2, W/2, C)
        x2 = x[:, 0::2, 1::2, :]  # (B, H/2, W/2, C)
        x3 = x[:, 1::2, 1::2, :]  # (B, H/2, W/2, C)

        x = torch.cat([x0, x1, x2, x3], dim=-1)  # (B, H/2, W/2, 4*C)
        x = self.norm(x) # (B, H/2, W/2, 4*C)
        x = self.reduction(x) # (B, H/2, W/2, 2*C)
        x = x.permute(0, 3, 1, 2).contiguous()  # (B, 2*C, H/2, W/2)
        return x

class Encoder(nn.Module):
    """
    Encoder pathway
    Input shape: (B, 3, H, W)
    Output shape: (B, H/32*W/32, 768) if input_size=512 and patch_size=4
    Also outputs three intermediate features for skip connections:
    to_haf_1: (B, 96, H/4, W/4)
    to_haf_2: (B, 192, H/8, W/8)
    to_haf_3: (B, 384, H/16, W/16)

    Workflow of encoder:
    (B, 3, H, W) - Patch partition-> (B, H/4, W/4, 48) - Linear embed -> (B, 96, H/4, W/4)
        -> Swin blocks (x2) -> (B, 96, H/4, W/4) -> to_haf_1
        -> Patch Merging -> (B, 192, H/8, W/8)
        -> Swin blocks (x2) -> (B, 192, H/8, W/8) -> to_haf_2
        -> Patch Merging -> (B, 384, H/16, W/16)
        -> Swin blocks (x2) -> (B, 384, H/16, W/16) -> to_haf_3
        -> Patch Merging -> (B, 768, H/32, W/32)
        -> Swin blocks (x2) -> (B, 768, H/32, W/32)

    Finally, flatten the spatial dimensions for next stage.
    Result shape: (B, 768, H/32*W/32) -
    -> 32x downsampling if input_size=512 and patch_size=4
    """
    def __init__(self, in_channels=CHANNELS, embed_dim=EMBEDDING_DIM, depths=DEPTHS, num_heads=NUM_HEADS, patch_size=PATCH_SIZE, input_size=INPUT_SIZE):
        super(Encoder, self).__init__()
        self.patch_partition = PatchPartition(patch_size=patch_size)
        self.linear_embed = LinearEmbed(in_channels * patch_size * patch_size, embed_dim)
        # There are 3 variables to hold the outputs for skip connections (will go to HAF blocks after each stage)
        self.to_haf_1 = None
        self.to_haf_2 = None
        self.to_haf_3 = None
        # Calculate resolution after patch partition
        current_resolution = input_size // patch_size  # 512 // 4 = 128
        self.layers = nn.ModuleList()

        for i in range(len(depths)):
            layer = nn.ModuleList()
            for j in range(depths[i]):
                block = SwinTransformerBlock(
                    dim=embed_dim * (2 ** i),
                    input_resolution=(current_resolution, current_resolution),  # Use actual resolution
                    num_heads=num_heads[i],
                    window_size=WINDOW_SIZE,
                    shift_size=0 if (j % 2 == 0) else WINDOW_SIZE // 2,
                    mlp_ratio=MLP_RATIO,
                    qkv_bias=QKV_BIAS,
                    attn_drop=ATTN_DROP_RATE,
                    drop_path=DROP_PATH_RATE,
                    norm_layer=nn.LayerNorm
                )
                layer.append(block)
            self.layers.append(layer)

            self.layers.append(PatchMerging(embed_dim * (2 ** i)))
            current_resolution = current_resolution // 2  # Resolution halves after patch merging

    def forward(self, x):
        # Assume that patch_size == 4
        x = self.patch_partition(x)  # (B, H/4, W/4, 48) - note: still (B, H, W, C) from patch partition
        x = self.linear_embed(x)  # (B, 96, H/4, W/4) - converted to channel-first

        for layer in self.layers:
            if isinstance(layer, nn.ModuleList):
                # Convert to (B, H, W, C) for Swin Transformer blocks
                B, C, H, W = x.shape
                x_swin = x.permute(0, 2, 3, 1).contiguous()  # (B, H, W, C) - convert for Swin blocks

                # Process Swin Transformer blocks
                for block in layer:
                    x_swin = block(x_swin)  # (B, H, W, C)

                # Convert back to channel-first
                x = x_swin.permute(0, 3, 1, 2).contiguous()  # (B, C, H, W) - back to channel-first

                if C == EMBEDDING_DIM:  # 96 channels
                    self.to_haf_1 = x  # (B, 96, H/4, W/4)
                elif C == EMBEDDING_DIM * 2:  # 192 channels
                    self.to_haf_2 = x  # (B, 192, H/8, W/8)
                elif C == EMBEDDING_DIM * 4:  # 384 channels
                    self.to_haf_3 = x  # (B, 384, H/16, W/16)

            else:
                # Process PatchMerging layer (expects (B, C, H, W))
                x = layer(x)  # (B, 2*C, H/2, W/2)

        # Flatten spatial dimensions for CBE
        B, C, H, W = x.shape  # (B, 768, H/32, W/32)
        x = x.view(B, C, -1)  # (B, 768, H/32*W/32)
        return x


#-------------------Skip Connection Blocks-------------------#
class HAF(nn.Module):
    """
    Hierarchical Attention Fusion (HAF) Block
    Input shape: x_decoder: (B, C, H, W);
                x_skip: (B, C, H, W)
    Output shape: (B, C, H, W)
    x_decoder -> swin transformer block
                    |
    x_skip ------> cat --> (B, 2C, H, W) --> conv 1x1 -> result
    """
    def __init__(self, dim, input_resolution : Tuple[int, int], num_heads=NUM_HEADS, embedding_dim=EMBEDDING_DIM):
        # dim is C in the docstring
        # resolution is (H, W)
        super(HAF, self).__init__()
        self.swin_block = None
        for i in range(2):
            self.swin_block = SwinTransformerBlock(
                dim=dim,
                input_resolution=input_resolution,
                num_heads=num_heads[0] if dim == embedding_dim else num_heads[1] if dim == embedding_dim * SCALE_FACTOR else num_heads[2],
                window_size=WINDOW_SIZE,
                shift_size=0 if (i % 2 == 0) else WINDOW_SIZE // 2,
                mlp_ratio=MLP_RATIO,
                qkv_bias=QKV_BIAS,
                attn_drop=ATTN_DROP_RATE,
                drop_path=DROP_PATH_RATE,
                norm_layer=nn.LayerNorm
            )
        self.conv1x1 = nn.Conv2d(2 * dim, dim, kernel_size=1, bias=False)

    def forward(self, x_decoder, x_skip):
        # Convert to (B, H, W, C) for SwinTransformerBlock
        x_decoder_swin = x_decoder.permute(0, 2, 3, 1).contiguous()  # (B, C, H, W) -> (B, H, W, C)
        x_skip_swin = x_skip.permute(0, 2, 3, 1).contiguous()  # (B, C, H, W) -> (B, H, W, C)

        x = self.swin_block(x_decoder_swin)  # (B, H, W, C) - Swin block output
        x = torch.cat([x, x_skip_swin], dim=-1)  # (B, H, W, 2C) - concatenate along channel dim
        x = x.permute(0, 3, 1, 2).contiguous()  # (B, H, W, 2C) -> (B, 2C, H, W)
        x = (self.conv1x1(x))  # (B, 2C, H, W) -> (B, C, H, W)
        return x


#-------------------Decoder Blocks-------------------#


class PatchExpanding(nn.Module):
    """
    Patch Expanding Layer that upsamples the input feature map using deconvolution.
    Input shape: (B, C, H, W)
    Output shape: (B, C/2, 2*H, 2*W)
    """
    def __init__(self, dim, dim_scale=SCALE_FACTOR):
        super(PatchExpanding, self).__init__()
        self.dim = dim
        self.dim_scale = dim_scale
        # Deconvolution layer: upsamples by 2x and reduces channels by half
        # self.deconv = nn.ConvTranspose2d(
        #     in_channels=dim,
        #     out_channels=dim // dim_scale,
        #     kernel_size=4,
        #     stride=2,
        #     padding=1,
        #     bias=False
        # )
        self.linear_layer = nn.Linear(dim, dim * dim_scale)
        self.norm = nn.LayerNorm(dim * dim_scale)
        # GroupNorm works better with channel-first format
        # self.norm = nn.GroupNorm(1, dim // dim_scale)

    def forward(self, x):
        """
        x: (B, C, H, W)
        """
        # x = self.deconv(x)  # (B, C, H, W) -> (B, C/2, 2*H, 2*W)
        x = x.permute(0, 2, 3, 1).contiguous()  # (B, C, H, W) -> (B, H, W, C)
        x = self.linear_layer(x) # (B, H, W, C) -> (B, H, W, C * dim_scale)
        x = self.norm(x)    # Normalize the upsampled features
        x = x.permute(0, 3, 1, 2).contiguous()  # (B, H, W, C*dim_scale) -> (B, C*dim_scale, H, W)
        x = rearrange(x, 'b (c h1 w1) h w -> b c (h h1) (w w1)', h1=self.dim_scale, w1=self.dim_scale)  # (B, C*dim_scale, H, W) -> (B, C/dim_scale,H * dim_scale, W * dim_scale)
        return x

class ACA(nn.Module):
    """
    Adaptive Contextual Aggregator (ACA) Block
    Input shape: (B, C, H, W)
    Output shape: (B, C, H, W)

    Workflow:
            --> Swin Transformer Block      |
            |
    (B, C, H, W) ->                      -->cat (B, 3C, H, W) --> conv 1 x 1 -> (B, C, H, W)
            | --> DeformableConv2d          |

    """
    def __init__(self, dim, input_resolution : Tuple[int, int], depth=2, num_heads=NUM_HEADS, embedding_dim=EMBEDDING_DIM):
        super(ACA, self).__init__()
        self.swin_layers = nn.ModuleList()
        for i in range(depth):
            swin_block = SwinTransformerBlock(
                dim=dim,
                input_resolution=input_resolution,
                num_heads=num_heads[0] if dim == embedding_dim else num_heads[1] if dim == embedding_dim * SCALE_FACTOR else num_heads[2],
                window_size=WINDOW_SIZE,
                shift_size=0 if (i % 2 == 0) else WINDOW_SIZE // 2,
                mlp_ratio=MLP_RATIO,
                qkv_bias=QKV_BIAS,
                attn_drop=ATTN_DROP_RATE,
                drop_path=DROP_PATH_RATE,
                norm_layer=nn.LayerNorm
            )
            self.swin_layers.append(swin_block)
        self.conv1x1 = nn.Conv2d(3 * dim, dim, kernel_size=1, bias=False)

    def forward(self, x_decoder, deform_group=1, deform_kernel_size=3):
        B, C, H, W = x_decoder.shape  # Input: (B, C, H, W)

        # Convert to (B, H, W, C) for SwinTransformerBlock
        x_decoder_swin = x_decoder.permute(0, 2, 3, 1).contiguous()  # (B, C, H, W) -> (B, H, W, C)

        x_swin = x_decoder_swin
        for layer in self.swin_layers:
            x_swin = layer(x_swin)  # (B, H, W, C) - Swin transformer processing

        # Deformable convolution (already in channel-first format)
        x_deform = deform_conv2d(input=x_decoder,  # Input: (B, C, H, W)
                                 offset=torch.zeros(B, 2*deform_group*deform_kernel_size*deform_kernel_size, H, W, device=x_decoder.device),
                                 weight=torch.ones(C, C // deform_group , deform_kernel_size, deform_kernel_size, device=x_decoder.device) / (deform_kernel_size*deform_kernel_size),
                                 padding=(deform_kernel_size//2, deform_kernel_size//2),)  # Output: (B, C, H, W)

        # Convert swin output to channel-first for concatenation
        x_swin = x_swin.permute(0, 3, 1, 2).contiguous()  # (B, H, W, C) -> (B, C, H, W)

        x = torch.cat([x_decoder, x_swin, x_deform], dim=1)  # (B, C, H, W) + (B, C, H, W) + (B, C, H, W) -> (B, 3C, H, W)
        x = (self.conv1x1(x))  # (B, 3C, H, W) -> (B, C, H, W)
        return x


class Decoder (nn.Module):
    """
    Decoder pathway
    Input shape: (B, 768, H/32 * W/32) if input_size=512, patch_size=4 and embedding_dim=96
    Output shape: (B, 48, H/2, W/2) if input_size=512, patch_size=4 and embedding_dim=96
    """
    def __init__(self, in_channels=SCALE_FACTOR**(len(DEPTHS))*EMBEDDING_DIM, depths=DEPTHS, input_size=INPUT_SIZE, patch_size=PATCH_SIZE, embedding_dim=EMBEDDING_DIM):
        """
        in_channels: channels of the input feature map (from encoder)
        """
        super(Decoder, self).__init__()
        self.layers = nn.ModuleList()
        self.to_haf_1 = None
        self.to_haf_2 = None
        self.to_haf_3 = None
        self.haf_1 = HAF(dim=embedding_dim,
                            input_resolution=(input_size // (patch_size), input_size // (patch_size)),
                            embedding_dim=embedding_dim)
        self.haf_2 = HAF(dim=embedding_dim * SCALE_FACTOR,
                            input_resolution=(input_size // (patch_size * SCALE_FACTOR), input_size // (patch_size * SCALE_FACTOR)),
                            embedding_dim=embedding_dim)
        self.haf_3 = HAF(dim=embedding_dim * SCALE_FACTOR**2,
                            input_resolution=(input_size // (patch_size * SCALE_FACTOR**2), input_size // (patch_size * SCALE_FACTOR**2)),
                            embedding_dim=embedding_dim)
        current_resolution =  input_size // (patch_size * (SCALE_FACTOR ** len(DEPTHS)))  # Starting resolution
        for depth in depths:
            self.layers.append(PatchExpanding(in_channels))
            in_channels //= 2  # Halve the channels after each upsampling
            current_resolution *= 2  # Double the resolution after each upsampling

            layer = nn.ModuleList()
            layer.append(ACA(in_channels, input_resolution=(current_resolution, current_resolution)))
            self.layers.append(layer)

        self.layers.append(PatchExpanding(in_channels))


    def forward(self, x, x_encode_haf_1, x_encode_haf_2, x_encode_haf_3):
        # x: (B, 768, H*W) - channel-first flattened from CBE
        B, C, HW = x.shape
        H = W = int(HW ** 0.5)  # H=W=16 for 512x512 input with 32x downsampling
        x = x.view(B, C, H, W)  # (B, 768, H*W) -> (B, 768, H/32, W/32)

        for layer in self.layers:
            if isinstance(layer, nn.ModuleList):
                # Process ACA blocks
                for block in layer:
                    B, C, H, W = x.shape
                    x = block(x)  # (B, C, H, W) - ACA processing
                if C == EMBEDDING_DIM * (SCALE_FACTOR ** 2):  # 384 channels
                    self.to_haf_3 = self.haf_3(x, x_encode_haf_3)  # (B, 384, H/16, W/16)
                elif C == EMBEDDING_DIM * SCALE_FACTOR:  # 192 channels
                    self.to_haf_2 = self.haf_2(x, x_encode_haf_2)  # (B, 192, H/8, W/8)
                elif C == EMBEDDING_DIM:  # 96 channels
                    self.to_haf_1 = self.haf_1(x, x_encode_haf_1)  # (B, 96, H/4, W/4)

            else: # PatchExpanding layer
                if C == EMBEDDING_DIM * (SCALE_FACTOR ** 2):  # 384 channels
                    x = 1/2*x + 1/2 * self.to_haf_3
                elif C == EMBEDDING_DIM * SCALE_FACTOR:  # 192 channels
                    x = 1/2*x + 1/2 * self.to_haf_2
                elif C == EMBEDDING_DIM:  # 96 channels
                    x = 1/2*x + 1/2 * self.to_haf_1
                x = layer(x)  # (B, C, H, W) -> (B, C/2, 2*H, 2*W) - upsampling
        return x  # (B, 48, H/2, W/2) - final decoder output


#-------------------Contextual Bottleneck Enhancer (CBE) Blocks-------------------#

class CBE(nn.Module):
    """
    Contextual Bottleneck Enhancer (CBE) Block based on shiftmlp concept.
    Input shape: (B, C, H*W) where C=768, H*W=16*16=256
    Output shape: (B, C, H*W)
    Workflow:
    x: (B, C, H*W) -> Reshape to (B, C, H, W)
    --> Pad (B, C, H+pad*2, W+pad*2)
    --> Split along width axis (B, C/shift_size, H+pad*2, W+pad*2) * shift_size
    --> Shift each split along width axis
    --> Concatenate (B, C, H+pad*2, W+pad*2)
    --> Reshape to (B, C, (H+pad*2)*(W+pad*2)) -> Transpose to (B, (H+pad*2)*(W+pad*2), C)
    --> Linear Projection (B, (H+pad*2)*(W+pad*2), token_embedding_dim)
    --> Reshape to (B* (H+pad*2), W+pad*2, token_embedding_dim) -> Transpose to (B* (H+pad*2), token_embedding_dim, W+pad*2)
    --> DWConv (B* (H+pad*2), token_embedding_dim, W+pad*2)
    --> Reshape back to (B, (H+pad*2)*(W+pad*2), token_embedding_dim) -> Transpose to (B, token_embedding_dim, (H+pad*2)*(W+pad*2))
    --> Gelu activation
    --> Linear Projection (B, (H+pad*2)*(W+pad*2), in_channels)
    --> Reshape to (B, in_channels, H+pad*2, W+pad*2)
    --> Reverse Shift and split
    --> Unpad
    --> Residual connection
    """
    def __init__(self, in_features=SCALE_FACTOR**(len(DEPTHS))*EMBEDDING_DIM, token_embedding_dim=TOKEN_EMBEDDING_DIM):
      super().__init__()


      # Using conv1d will give bad result than linear
      self.shifted_proj1 = nn.Linear(in_features, token_embedding_dim)
      self.depthwise_conv = nn.Conv2d(token_embedding_dim, token_embedding_dim, kernel_size=3, stride=1, padding=1, groups=token_embedding_dim, bias=True)
      self.gelu = nn.GELU()

      self.norm = nn.GroupNorm(1, token_embedding_dim)  # GroupNorm works better with (B, C, H*W)
      self.feature_proj = nn.Linear(token_embedding_dim, in_features)
      self.drop = nn.Dropout(0.1)

    def _init_weights(self, m):
      if isinstance(m, nn.Linear):
        trunc_normal_(m.weight, std=.02)
        if isinstance(m, nn.Linear) and m.bias is not None:
            nn.init.constant_(m.bias, 0)
      elif isinstance(m, nn.LayerNorm):
        nn.init.constant_(m.bias, 0)
        nn.init.constant_(m.weight, 1.0)
      elif isinstance(m, nn.Conv2d):
        fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
        fan_out //= m.groups
        m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))
        if m.bias is not None:
            m.bias.data.zero_()
    def shift_along(self, x, dim=3, shift_size=SHIFT_SIZE):
      """
      Shift the feature map along a specified axis.
      Input: x (B, C, H*W) - channel-first flattened format
      Output: x (B, C, H*W) - shifted features
      """
      B, C, HW = x.shape
      H = W = int(HW ** 0.5)  # Assuming square input for simplicity
      x = x.view(B, C, H, W)  # (B, C, H*W) -> (B, C, H, W)
      x = torch.roll(x, shifts=shift_size, dims=dim)  # Shift along specified axis
      x = x.view(B, C, HW)  # (B, C, H, W) -> (B, C, H*W)
      return x
    def forward(self, x):

      x_shifted = self.shift_along(x, dim=3)  # (B, 768, H*W) - width-shifted features
      x_shifted = self.shifted_proj1(x_shifted.transpose(1,2)).transpose(1,2)  # (B, 768, H*W) -> (B, H*W, token_embedding_dim) -> (B, token_embedding_dim, H*W)
      B, C, HW = x_shifted.shape
      # x_shifted = self.shifted_proj1(x_shifted)  # (B, 768, H*W) -> (B, token_embedding_dim, H*W)
      # B, C, HW = x_shifted.shape

      H = W = int(HW ** 0.5)  # H=W=16 for 512x512 input with 32x downsampling

      x_shifted = x_shifted.view(B, C, H, W)  # (B, token_embedding_dim, H*W) -> (B, token_embedding_dim, H, W)
      x_shifted = self.depthwise_conv(x_shifted)  # (B, token_embedding_dim, H, W) - spatial processing
      x_shifted = self.gelu(x_shifted)  # (B, token_embedding_dim, H, W) - activation
      x_shifted = x_shifted.view(B, C, HW)  # (B, token_embedding_dim, H, W) -> (B, token_embedding_dim, H*W)
      x_shifted = self.norm(x_shifted)  # (B, token_embedding_dim, H*W) - group normalization

      x_shifted = self.shift_along(x_shifted, dim=2, shift_size=-SHIFT_SIZE)  # (B, token_embedding_dim, H*W) - reverse height shift
      x_shifted = self.feature_proj(x_shifted.transpose(1,2)).transpose(1,2)  # (B, token_embedding_dim, H*W) -> (B, in_features, H*W)
      # x_shifted = self.feature_proj(x_shifted)  # (B, token_embedding_dim, H*W) -> (B, in_channels, H*W)

      x_drop = self.drop(x_shifted)  # (B, 768, H*W) - dropout
      x = x + x_drop  # Residual connection
      return x  # (B, 768, H*W) - enhanced features
class SwinHAFNet(nn.Module):
    """
    SWIN-HAFNet model
    Input shape: (B, 3, H, W)
    Output shape: (B, 2, H/2, W/2) - logits for 2 classes
    """
    def __init__(self, in_channels=CHANNELS, num_classes=NUM_CLASSES,
                 embedding_dim=EMBEDDING_DIM, depths=DEPTHS, num_heads=NUM_HEADS,
                 patch_size=PATCH_SIZE, input_size=INPUT_SIZE):
        super(SwinHAFNet, self).__init__()
        self.encoder = Encoder(in_channels, embedding_dim, depths, num_heads, patch_size, input_size)
        self.cbe = CBE(in_features=SCALE_FACTOR**(len(depths))*embedding_dim)
        self.decoder = Decoder(in_channels=embedding_dim * (SCALE_FACTOR**len(depths)), depths=depths, input_size=input_size, patch_size=patch_size, embedding_dim=embedding_dim)
        # self.final_conv = nn.Conv2d(embedding_dim // 2, num_classes, kernel_size=1)
        self.final_linear = nn.Linear(embedding_dim // 2, num_classes)
        # use bilinear interpolation for upsampling (or deconv instead)
        self.final_upsampling = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)

    def forward(self, x):
        # Encoder
        x_encoded = self.encoder(x)  # (B, 768, H/32*W/32)
        to_haf_1 = self.encoder.to_haf_1 # (B, 96, H/4, W/4)
        to_haf_2 = self.encoder.to_haf_2 # (B, 192, H/8, W/8)
        to_haf_3 = self.encoder.to_haf_3 # (B, 384, H/16, W/16)

        # CBE
        x_cbe = self.cbe(x_encoded)
        # Decoder
        x_decoded = self.decoder(x_cbe, to_haf_1, to_haf_2, to_haf_3) # (B, 48, H/2, W/2)

        # Final convolution to get class logits
        # x_decoded = F.relu(self.final_conv(x_decoded)) # (B, 2, H/2, W/2)
        x_decoded = x_decoded.permute(0,2,3,1).contiguous()
        x_decoded = self.final_linear(x_decoded)
        x_decoded = x_decoded.permute(0, 3, 1, 2).contiguous()  # (B, H/2, W/2, NUM_CLASSES) -> (B, NUM_CLASSES, H/2, W/2)
        logits = self.final_upsampling(x_decoded) # (B, 2, H, W)
        return logits